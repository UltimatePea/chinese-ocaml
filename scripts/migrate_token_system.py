#!/usr/bin/env python3
"""
Tokenç³»ç»Ÿé‡ç»„è¿ç§»è„šæœ¬

åŸºäºDeltaä¸“å‘˜Issue #1359çš„åˆ†æï¼Œå°†åˆ†æ•£çš„tokenç›¸å…³æ–‡ä»¶æ•´åˆåˆ°ç»Ÿä¸€çš„ç›®å½•ç»“æ„ä¸­

Author: Alpha, ä¸»è¦å®ç°ä¸“å‘˜
"""

import os
import shutil
import glob
from pathlib import Path

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
SRC_DIR = PROJECT_ROOT / "src"
NEW_TOKEN_DIR = SRC_DIR / "token_system_unified"

def ensure_dir(path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    path.mkdir(parents=True, exist_ok=True)

def migrate_token_files():
    """è¿ç§»åˆ†æ•£çš„tokenæ–‡ä»¶åˆ°ç»Ÿä¸€ç›®å½•ç»“æ„"""
    
    print("ğŸš€ å¼€å§‹Tokenç³»ç»Ÿé‡ç»„è¿ç§»...")
    
    # ç¡®ä¿æ–°ç›®å½•ç»“æ„å­˜åœ¨
    for subdir in ["core", "conversion", "mapping", "utils"]:
        ensure_dir(NEW_TOKEN_DIR / subdir)
    
    # è¿ç§»è®¡åˆ’
    migration_plan = {
        # æ ¸å¿ƒTokenå®šä¹‰å’Œç±»å‹ -> core/
        "core": [
            "src/token_types.ml",
            "src/token_types.mli", 
            "src/unified_token_core.ml",
            "src/unified_token_core.mli",
            "src/token_types_core.ml",
            "src/token_types_core.mli",
            "src/tokens/core/*",
            "src/token_system/core/*"
        ],
        
        # Tokenè½¬æ¢å’Œå…¼å®¹æ€§ -> conversion/
        "conversion": [
            "src/token_conversion_*.ml",
            "src/token_conversion_*.mli",
            "src/token_compatibility*.ml", 
            "src/token_compatibility*.mli",
            "src/tokens/conversion/*",
            "src/token_system/conversion/*",
            "src/token_system/compatibility/*"
        ],
        
        # Tokenæ˜ å°„å’Œæ³¨å†Œ -> mapping/
        "mapping": [
            "src/lexer/token_mapping/*",
            "src/tokens/mapping/*",
            "src/unified_token_registry.ml",
            "src/unified_token_registry.mli"
        ],
        
        # Tokenå·¥å…·å’Œè¾…åŠ©åŠŸèƒ½ -> utils/
        "utils": [
            "src/token_string_converter.ml",
            "src/token_string_converter.mli",
            "src/token_utils_core.ml", 
            "src/token_utils_core.mli",
            "src/token_dispatcher.ml",
            "src/parser_expressions_token_reducer.ml",
            "src/parser_expressions_token_reducer.mli",
            "src/tokens/delimiter_tokens.ml",
            "src/tokens/delimiter_tokens.mli",
            "src/tokens/identifier_tokens.ml",
            "src/tokens/identifier_tokens.mli",
            "src/tokens/keyword_tokens.ml",
            "src/tokens/keyword_tokens.mli",
            "src/tokens/literal_tokens.ml",
            "src/tokens/literal_tokens.mli",
            "src/tokens/operator_tokens.ml",
            "src/tokens/operator_tokens.mli",
            "src/tokens/natural_language_tokens.ml",
            "src/tokens/natural_language_tokens.mli",
            "src/tokens/poetry_tokens.ml",
            "src/tokens/poetry_tokens.mli",
            "src/tokens/wenyan_tokens.ml",
            "src/tokens/wenyan_tokens.mli",
            "src/tokens/unified_tokens.ml",
            "src/tokens/unified_tokens.mli",
            "src/token_system/utils/*"
        ]
    }
    
    # è®°å½•è¿ç§»æƒ…å†µ
    migration_log = []
    
    for target_dir, patterns in migration_plan.items():
        target_path = NEW_TOKEN_DIR / target_dir
        print(f"\nğŸ“ è¿ç§»åˆ° {target_dir}/")
        
        for pattern in patterns:
            # å¤„ç†é€šé…ç¬¦æ¨¡å¼
            if "*" in pattern:
                files = glob.glob(str(PROJECT_ROOT / pattern))
            else:
                files = [str(PROJECT_ROOT / pattern)]
            
            for file_path in files:
                if os.path.exists(file_path) and os.path.isfile(file_path):
                    filename = os.path.basename(file_path)
                    target_file = target_path / filename
                    
                    # é¿å…é‡å¤æ–‡ä»¶
                    if target_file.exists():
                        print(f"   âš ï¸  è·³è¿‡å·²å­˜åœ¨æ–‡ä»¶: {filename}")
                        continue
                    
                    try:
                        shutil.copy2(file_path, target_file)
                        migration_log.append(f"MOVED: {file_path} -> {target_file}")
                        print(f"   âœ… {filename}")
                    except Exception as e:
                        print(f"   âŒ è¿ç§»å¤±è´¥ {filename}: {e}")
                        migration_log.append(f"ERROR: {file_path} -> {e}")
    
    # ä¿å­˜è¿ç§»æ—¥å¿—
    log_file = PROJECT_ROOT / "doc" / "migration_log.txt"
    with open(log_file, "w", encoding="utf-8") as f:
        f.write("Tokenç³»ç»Ÿé‡ç»„è¿ç§»æ—¥å¿—\n")
        f.write("=" * 50 + "\n\n")
        for entry in migration_log:
            f.write(entry + "\n")
    
    print(f"\nğŸ“ è¿ç§»æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
    
    # åˆ›å»ºæ–°çš„duneé…ç½®
    create_unified_dune_configs()
    
    print("\nğŸ‰ Tokenç³»ç»Ÿé‡ç»„è¿ç§»å®Œæˆï¼")

def create_unified_dune_configs():
    """åˆ›å»ºç»Ÿä¸€çš„duneé…ç½®æ–‡ä»¶"""
    
    # ä¸»token_system_unified duneé…ç½®
    main_dune = """(library
 (public_name chinese-ocaml.token_system_unified)
 (name token_system_unified)
 (libraries 
   token_system_unified.core
   token_system_unified.conversion
   token_system_unified.mapping
   token_system_unified.utils))
"""
    
    # coreå­æ¨¡å—duneé…ç½®
    core_dune = """(library
 (public_name chinese-ocaml.token_system_unified.core)
 (name token_system_unified_core)
 (libraries))
"""
    
    # conversionå­æ¨¡å—duneé…ç½®
    conversion_dune = """(library
 (public_name chinese-ocaml.token_system_unified.conversion)
 (name token_system_unified_conversion)  
 (libraries token_system_unified.core))
"""
    
    # mappingå­æ¨¡å—duneé…ç½®
    mapping_dune = """(library
 (public_name chinese-ocaml.token_system_unified.mapping)
 (name token_system_unified_mapping)
 (libraries token_system_unified.core))
"""
    
    # utilså­æ¨¡å—duneé…ç½®
    utils_dune = """(library
 (public_name chinese-ocaml.token_system_unified.utils)
 (name token_system_unified_utils)
 (libraries 
   token_system_unified.core
   token_system_unified.conversion
   token_system_unified.mapping))
"""
    
    # å†™å…¥duneé…ç½®æ–‡ä»¶
    configs = {
        NEW_TOKEN_DIR / "dune": main_dune,
        NEW_TOKEN_DIR / "core" / "dune": core_dune,
        NEW_TOKEN_DIR / "conversion" / "dune": conversion_dune,
        NEW_TOKEN_DIR / "mapping" / "dune": mapping_dune,
        NEW_TOKEN_DIR / "utils" / "dune": utils_dune
    }
    
    for path, content in configs.items():
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "w") as f:
            f.write(content)
        print(f"   âœ… åˆ›å»ºduneé…ç½®: {path}")

def analyze_current_structure():
    """åˆ†æå½“å‰tokenæ–‡ä»¶åˆ†å¸ƒæƒ…å†µ"""
    print("ğŸ” åˆ†æå½“å‰Tokenæ–‡ä»¶åˆ†å¸ƒ...")
    
    token_files = []
    for root, dirs, files in os.walk(SRC_DIR):
        for file in files:
            if "token" in file.lower() and (file.endswith(".ml") or file.endswith(".mli")):
                full_path = os.path.join(root, file)
                rel_path = os.path.relpath(full_path, PROJECT_ROOT)
                token_files.append(rel_path)
    
    print(f"ğŸ“Š å‘ç° {len(token_files)} ä¸ªtokenç›¸å…³æ–‡ä»¶")
    
    # æŒ‰ç›®å½•åˆ†ç»„
    by_directory = {}
    for file_path in token_files:
        directory = os.path.dirname(file_path)
        if directory not in by_directory:
            by_directory[directory] = []
        by_directory[directory].append(os.path.basename(file_path))
    
    print("\nğŸ“ ç›®å½•åˆ†å¸ƒ:")
    for directory, files in sorted(by_directory.items()):
        print(f"   {directory}/ ({len(files)} files)")
        for file in sorted(files)[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶
            print(f"     - {file}")
        if len(files) > 3:
            print(f"     - ... è¿˜æœ‰ {len(files)-3} ä¸ªæ–‡ä»¶")

if __name__ == "__main__":
    print("ğŸš€ Tokenç³»ç»Ÿé‡ç»„è¿ç§»è„šæœ¬")
    print("=" * 50)
    
    # é¦–å…ˆåˆ†æå½“å‰ç»“æ„
    analyze_current_structure()
    
    # è‡ªåŠ¨æ‰§è¡Œè¿ç§»
    print("\nğŸš€ è‡ªåŠ¨æ‰§è¡Œè¿ç§»...")
    migrate_token_files()