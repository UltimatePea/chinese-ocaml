#!/usr/bin/env python3
"""
éª†è¨€é¡¹ç›®æµ‹è¯•è¦†ç›–çŽ‡åˆ†æžå·¥å…· - Fix #732
åˆ†æžé¡¹ç›®çš„æµ‹è¯•è¦†ç›–æƒ…å†µå¹¶ç”Ÿæˆæ”¹è¿›å»ºè®®
"""

import os
import glob
import json
from collections import defaultdict, Counter
from pathlib import Path

class TestCoverageAnalyzer:
    def __init__(self, project_root):
        self.project_root = Path(project_root)
        self.src_dir = self.project_root / "src"
        self.test_dir = self.project_root / "test"
        
    def analyze_source_files(self):
        """åˆ†æžæºæ–‡ä»¶ç»“æž„"""
        source_files = {}
        
        for ml_file in self.src_dir.glob("**/*.ml"):
            rel_path = ml_file.relative_to(self.src_dir)
            module_name = ml_file.stem
            
            # èŽ·å–æ–‡ä»¶å¤§å°å’Œè¡Œæ•°
            try:
                with open(ml_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    line_count = len(lines)
                    
                source_files[str(rel_path)] = {
                    'module': module_name,
                    'path': str(ml_file),
                    'lines': line_count,
                    'size': ml_file.stat().st_size,
                    'category': self.categorize_module(rel_path)
                }
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {ml_file}: {e}")
                
        return source_files
    
    def analyze_test_files(self):
        """åˆ†æžæµ‹è¯•æ–‡ä»¶ç»“æž„"""
        test_files = {}
        
        for test_file in self.test_dir.glob("**/*.ml"):
            rel_path = test_file.relative_to(self.test_dir)
            test_name = test_file.stem
            
            try:
                with open(test_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    lines = content.split('\n')
                    
                test_files[str(rel_path)] = {
                    'name': test_name,
                    'path': str(test_file),
                    'lines': len(lines),
                    'content': content,
                    'tested_modules': self.extract_tested_modules(content)
                }
            except Exception as e:
                print(f"è¯»å–æµ‹è¯•æ–‡ä»¶å¤±è´¥ {test_file}: {e}")
                
        return test_files
    
    def categorize_module(self, rel_path):
        """æ ¹æ®è·¯å¾„åˆ†ç±»æ¨¡å—"""
        path_str = str(rel_path)
        
        if 'lexer' in path_str or 'token' in path_str:
            return 'è¯æ³•åˆ†æž'
        elif 'parser' in path_str:
            return 'è¯­æ³•åˆ†æž'
        elif 'semantic' in path_str:
            return 'è¯­ä¹‰åˆ†æž'
        elif 'types' in path_str:
            return 'ç±»åž‹ç³»ç»Ÿ'
        elif 'poetry' in path_str:
            return 'è¯—è¯ç¼–ç¨‹'
        elif 'error' in path_str:
            return 'é”™è¯¯å¤„ç†'
        elif 'codegen' in path_str or 'c_' in path_str:
            return 'ä»£ç ç”Ÿæˆ'
        elif 'builtin' in path_str:
            return 'å†…ç½®å‡½æ•°'
        elif 'config' in path_str:
            return 'é…ç½®ç®¡ç†'
        elif 'unicode' in path_str or 'utf8' in path_str:
            return 'Unicodeå¤„ç†'
        else:
            return 'å…¶ä»–'
    
    def extract_tested_modules(self, content):
        """ä»Žæµ‹è¯•å†…å®¹ä¸­æå–è¢«æµ‹è¯•çš„æ¨¡å—"""
        modules = set()
        
        # æŸ¥æ‰¾æ¨¡å—å¼•ç”¨
        lines = content.split('\n')
        for line in lines:
            if 'open' in line and 'Yyocamlc_lib' in line:
                modules.add('Yyocamlc_lib')
            if any(mod in line for mod in ['Lexer.', 'Parser.', 'Semantic.', 'Types.']):
                for mod in ['Lexer', 'Parser', 'Semantic', 'Types']:
                    if f'{mod}.' in line:
                        modules.add(mod)
                        
        return list(modules)
    
    def calculate_coverage_stats(self, source_files, test_files):
        """è®¡ç®—è¦†ç›–çŽ‡ç»Ÿè®¡"""
        category_stats = defaultdict(lambda: {'source_count': 0, 'test_count': 0, 'covered_modules': []})
        
        # ç»Ÿè®¡æºæ–‡ä»¶
        for file_info in source_files.values():
            category = file_info['category']
            category_stats[category]['source_count'] += 1
            
        # ç»Ÿè®¡æµ‹è¯•æ–‡ä»¶è¦†ç›–æƒ…å†µ
        for test_info in test_files.values():
            test_name = test_info['name'].lower()
            
            # åŸºäºŽæµ‹è¯•æ–‡ä»¶åè¿›è¡Œåˆ†ç±»æ˜ å°„
            if 'token' in test_name or 'lexer' in test_name:
                category_stats['è¯æ³•åˆ†æž']['test_count'] += 1
            elif 'unicode' in test_name or 'utf8' in test_name:
                category_stats['Unicodeå¤„ç†']['test_count'] += 1
            elif 'parser' in test_name:
                category_stats['è¯­æ³•åˆ†æž']['test_count'] += 1
            elif 'semantic' in test_name:
                category_stats['è¯­ä¹‰åˆ†æž']['test_count'] += 1
            elif 'types' in test_name:
                category_stats['ç±»åž‹ç³»ç»Ÿ']['test_count'] += 1
            elif 'poetry' in test_name:
                category_stats['è¯—è¯ç¼–ç¨‹']['test_count'] += 1
            elif 'error' in test_name:
                category_stats['é”™è¯¯å¤„ç†']['test_count'] += 1
            elif 'codegen' in test_name or 'c_' in test_name:
                category_stats['ä»£ç ç”Ÿæˆ']['test_count'] += 1
            elif 'builtin' in test_name:
                category_stats['å†…ç½®å‡½æ•°']['test_count'] += 1
            elif 'config' in test_name:
                category_stats['é…ç½®ç®¡ç†']['test_count'] += 1
            else:
                category_stats['å…¶ä»–']['test_count'] += 1
                    
        return dict(category_stats)
    
    def generate_coverage_report(self):
        """ç”Ÿæˆè¦†ç›–çŽ‡æŠ¥å‘Š"""
        print("ðŸ” éª†è¨€é¡¹ç›®æµ‹è¯•è¦†ç›–çŽ‡åˆ†æžæŠ¥å‘Š - Fix #732")
        print("=" * 60)
        
        source_files = self.analyze_source_files()
        test_files = self.analyze_test_files()
        coverage_stats = self.calculate_coverage_stats(source_files, test_files)
        
        print(f"\nðŸ“Š åŸºç¡€ç»Ÿè®¡:")
        print(f"  æºæ–‡ä»¶æ€»æ•°: {len(source_files)}")
        print(f"  æµ‹è¯•æ–‡ä»¶æ€»æ•°: {len(test_files)}")
        print(f"  æ•´ä½“è¦†ç›–çŽ‡: {len(test_files)/len(source_files)*100:.1f}%")
        
        print(f"\nðŸ“ˆ åˆ†ç±»è¦†ç›–çŽ‡ç»Ÿè®¡:")
        for category, stats in sorted(coverage_stats.items()):
            source_count = stats['source_count']
            test_count = stats['test_count']
            coverage = (test_count / source_count * 100) if source_count > 0 else 0
            print(f"  {category:12} | æºæ–‡ä»¶: {source_count:3d} | æµ‹è¯•: {test_count:3d} | è¦†ç›–çŽ‡: {coverage:5.1f}%")
        
        print(f"\nðŸŽ¯ æ ¸å¿ƒæ¨¡å—è¦†ç›–çŽ‡:")
        core_modules = ['è¯æ³•åˆ†æž', 'è¯­æ³•åˆ†æž', 'è¯­ä¹‰åˆ†æž', 'ç±»åž‹ç³»ç»Ÿ']
        for module in core_modules:
            if module in coverage_stats:
                stats = coverage_stats[module]
                coverage = (stats['test_count'] / stats['source_count'] * 100) if stats['source_count'] > 0 else 0
                status = "âœ…" if coverage >= 70 else "âš ï¸" if coverage >= 40 else "âŒ"
                print(f"  {status} {module}: {coverage:.1f}%")
        
        print(f"\nðŸŽ¨ è¯—è¯ç¼–ç¨‹ç‰¹è‰²è¦†ç›–çŽ‡:")
        poetry_categories = ['è¯—è¯ç¼–ç¨‹', 'é”™è¯¯å¤„ç†']
        for category in poetry_categories:
            if category in coverage_stats:
                stats = coverage_stats[category]
                coverage = (stats['test_count'] / stats['source_count'] * 100) if stats['source_count'] > 0 else 0
                status = "âœ…" if coverage >= 60 else "âš ï¸" if coverage >= 30 else "âŒ"
                print(f"  {status} {category}: {coverage:.1f}%")
        
        self.generate_improvement_suggestions(coverage_stats)
        
        return {
            'source_files': len(source_files),
            'test_files': len(test_files),
            'coverage_stats': coverage_stats,
            'overall_coverage': len(test_files)/len(source_files)*100
        }
    
    def generate_improvement_suggestions(self, coverage_stats):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        print(f"\nðŸ’¡ æµ‹è¯•è¦†ç›–çŽ‡æ”¹è¿›å»ºè®®:")
        
        # æŒ‰ä¼˜å…ˆçº§æŽ’åºç±»åˆ«
        priority_order = ['è¯æ³•åˆ†æž', 'è¯­æ³•åˆ†æž', 'è¯­ä¹‰åˆ†æž', 'ç±»åž‹ç³»ç»Ÿ', 'é”™è¯¯å¤„ç†', 'è¯—è¯ç¼–ç¨‹']
        
        for category in priority_order:
            if category in coverage_stats:
                stats = coverage_stats[category]
                coverage = (stats['test_count'] / stats['source_count'] * 100) if stats['source_count'] > 0 else 0
                
                if coverage < 70:
                    gap = 70 - coverage
                    needed_tests = int(stats['source_count'] * 0.7 - stats['test_count'])
                    priority = "ðŸ”¥ é«˜ä¼˜å…ˆçº§" if category in ['è¯æ³•åˆ†æž', 'è¯­æ³•åˆ†æž', 'è¯­ä¹‰åˆ†æž', 'ç±»åž‹ç³»ç»Ÿ'] else "ðŸ“ˆ ä¸­ä¼˜å…ˆçº§"
                    
                    print(f"  {priority} - {category}:")
                    print(f"    å½“å‰è¦†ç›–çŽ‡: {coverage:.1f}% (ç›®æ ‡: 70%)")
                    print(f"    å»ºè®®æ–°å¢žæµ‹è¯•: {needed_tests} ä¸ª")
                    print(f"    æ”¹è¿›å¹…åº¦: +{gap:.1f}%")
        
        print(f"\nðŸš€ ç¬¬ä¸€é˜¶æ®µå®žæ–½å»ºè®®:")
        print(f"  1. ä¼˜å…ˆå®Œå–„æ ¸å¿ƒç¼–è¯‘å™¨æ¨¡å—æµ‹è¯• (è¯æ³•ã€è¯­æ³•ã€è¯­ä¹‰ã€ç±»åž‹)")
        print(f"  2. åŠ å¼ºé”™è¯¯å¤„ç†ç³»ç»Ÿæµ‹è¯•è¦†ç›–")
        print(f"  3. æ‰©å±•è¯—è¯ç¼–ç¨‹ç‰¹è‰²åŠŸèƒ½æµ‹è¯•")
        print(f"  4. å»ºç«‹ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•å¥—ä»¶")

def main():
    analyzer = TestCoverageAnalyzer(os.getcwd())
    results = analyzer.generate_coverage_report()
    
    # ä¿å­˜ç»“æžœåˆ°æ–‡ä»¶
    output_file = "doc/analysis/æµ‹è¯•è¦†ç›–çŽ‡åˆ†æžç»“æžœ_Fix_732.json"
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nðŸ“„ è¯¦ç»†ç»“æžœå·²ä¿å­˜è‡³: {output_file}")

if __name__ == "__main__":
    main()