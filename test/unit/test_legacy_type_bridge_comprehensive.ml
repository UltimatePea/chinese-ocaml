(** éª†è¨€ç¼–è¯‘å™¨ - Legacy Type Bridge ç»¼åˆæµ‹è¯•å¥—ä»¶
    
    é’ˆå¯¹Issue #1357ä¸­Deltaä¸“å‘˜æŒ‡å‡ºçš„æµ‹è¯•è¦†ç›–ç‡ä¸è¶³é—®é¢˜ï¼Œ
    ä¸ºlegacy_type_bridge.mlæ¨¡å—æä¾›å®Œæ•´çš„æµ‹è¯•è¦†ç›–ã€‚
    
    @author Echo, æµ‹è¯•å·¥ç¨‹å¸ˆ
    @version 1.0
    @since 2025-07-26
    @issue #1357 *)

open Alcotest
open Token_system_core.Token_types
open Token_system_compatibility.Legacy_type_bridge

(** {1 æµ‹è¯•è¾…åŠ©å·¥å…·} *)

(** ç”Ÿæˆæµ‹è¯•ç”¨çš„ä½ç½®ä¿¡æ¯ *)
let make_test_position line column offset =
  { line; column; offset }

(** æ–­è¨€ä¸¤ä¸ªTokenç›¸ç­‰ *)
let token_testable = testable (fun fmt _ -> Format.fprintf fmt "Token") (=)

(** ç”Ÿæˆå¤§è§„æ¨¡æµ‹è¯•æ•°æ® *)
let generate_test_integers n =
  List.init n (fun i -> i * 7 + 3) (* ç”Ÿæˆéå¹³å‡¡çš„æ•´æ•°åºåˆ— *)

(** ç”Ÿæˆè¾¹ç•Œå€¼æµ‹è¯•æ•°æ® *)
let boundary_integers = [
  0; 1; -1; 
  max_int; min_int;
  42; -42;
  1000; -1000
]

let boundary_floats = [
  0.0; 1.0; -1.0;
  3.14159; -3.14159;
  1e10; 1e-10;
  infinity; neg_infinity
]

let test_strings = [
  ""; "hello"; "world";
  "ä¸­æ–‡æµ‹è¯•"; "ğŸ¯æµ‹è¯•";
  "å¸¦ç©ºæ ¼çš„å­—ç¬¦ä¸²";
  "ç‰¹æ®Šå­—ç¬¦!@#$%^&*()";
  String.make 1000 'x' (* é•¿å­—ç¬¦ä¸² *)
]

let chinese_numbers = [
  "ä¸€"; "äºŒ"; "ä¸‰"; "å››"; "äº”";
  "å…­"; "ä¸ƒ"; "å…«"; "ä¹"; "å";
  "ä¸€åäºŒ"; "äºŒåä¸‰"; "ä¸€ç™¾";
  "ä¸‰ç‚¹ä¸€å››"; "é›¶ç‚¹äº”"
]

(** {1 åŸºç¡€ç±»å‹è½¬æ¢å‡½æ•°æµ‹è¯•} *)

(** æ•´æ•°Tokenè½¬æ¢æµ‹è¯• *)
let test_convert_int_token () =
  (* åŸºç¡€æ•´æ•°è½¬æ¢ *)
  let result = convert_int_token 42 in
  check (testable (fun fmt -> function IntToken i -> Format.fprintf fmt "IntToken %d" i | _ -> Format.fprintf fmt "Other") (=)) "convert_int_token basic" (IntToken 42) result;
  
  (* è¾¹ç•Œå€¼æµ‹è¯• *)
  List.iter (fun i ->
    let result = convert_int_token i in
    check (testable (fun fmt -> function IntToken i -> Format.fprintf fmt "IntToken %d" i | _ -> Format.fprintf fmt "Other") (=)) ("convert_int_token boundary " ^ string_of_int i) (IntToken i) result
  ) boundary_integers

(** æµ®ç‚¹æ•°Tokenè½¬æ¢æµ‹è¯• *)
let test_convert_float_token _ =
  (* åŸºç¡€æµ®ç‚¹æ•°è½¬æ¢ *)
  let result = convert_float_token 3.14 in
  assert_equal (FloatToken 3.14) result;
  
  (* è¾¹ç•Œå€¼æµ‹è¯• *)
  List.iter (fun f ->
    let result = convert_float_token f in
    assert_equal (FloatToken f) result
  ) boundary_floats;
  
  (* ç‰¹æ®Šæµ®ç‚¹å€¼æµ‹è¯• *)
  let nan_result = convert_float_token nan in
  match nan_result with
  | FloatToken f when classify_float f = FP_nan -> ()
  | _ -> assert_failure "NaN conversion failed"

(** å­—ç¬¦ä¸²Tokenè½¬æ¢æµ‹è¯• *)
let test_convert_string_token _ =
  List.iter (fun s ->
    let result = convert_string_token s in
    assert_equal (StringToken s) result
  ) test_strings

(** å¸ƒå°”Tokenè½¬æ¢æµ‹è¯• *)  
let test_convert_bool_token _ =
  let true_result = convert_bool_token true in
  assert_equal (BoolToken true) true_result;
  
  let false_result = convert_bool_token false in
  assert_equal (BoolToken false) false_result

(** ä¸­æ–‡æ•°å­—Tokenè½¬æ¢æµ‹è¯• *)
let test_convert_chinese_number_token _ =
  List.iter (fun cn ->
    let result = convert_chinese_number_token cn in
    assert_equal (ChineseNumberToken cn) result
  ) chinese_numbers

(** {1 æ ‡è¯†ç¬¦è½¬æ¢å‡½æ•°æµ‹è¯•} *)

let test_identifiers = [
  "hello"; "world"; "test_var";
  "CamelCase"; "snake_case"; "_private";
  "ä¸­æ–‡å˜é‡"; "æ··åˆ_Chinese_æ ‡è¯†ç¬¦";
  "a"; String.make 100 'x' (* é•¿æ ‡è¯†ç¬¦ *)
]

(** ç®€å•æ ‡è¯†ç¬¦è½¬æ¢æµ‹è¯• *)
let test_convert_simple_identifier _ =
  List.iter (fun id ->
    let result = convert_simple_identifier id in
    assert_equal (SimpleIdentifier id) result
  ) test_identifiers

(** å¼•ç”¨æ ‡è¯†ç¬¦è½¬æ¢æµ‹è¯• *)
let test_convert_quoted_identifier _ =
  List.iter (fun id ->
    let result = convert_quoted_identifier id in
    assert_equal (QuotedIdentifierToken id) result
  ) test_identifiers

(** ç‰¹æ®Šæ ‡è¯†ç¬¦è½¬æ¢æµ‹è¯• *)
let test_convert_special_identifier _ =
  List.iter (fun id ->
    let result = convert_special_identifier id in
    assert_equal (IdentifierTokenSpecial id) result
  ) test_identifiers

(** {1 æ ¸å¿ƒå…³é”®å­—è½¬æ¢å‡½æ•°æµ‹è¯•} *)

(** å…³é”®å­—è½¬æ¢æµ‹è¯• *)
let test_keyword_conversions _ =
  assert_equal LetKeyword (convert_let_keyword ());
  assert_equal FunKeyword (convert_fun_keyword ());
  assert_equal IfKeyword (convert_if_keyword ());
  assert_equal ThenKeyword (convert_then_keyword ());
  assert_equal ElseKeyword (convert_else_keyword ())

(** {1 æ“ä½œç¬¦è½¬æ¢å‡½æ•°æµ‹è¯•} *)

(** æ“ä½œç¬¦è½¬æ¢æµ‹è¯• *)
let test_operator_conversions _ =
  assert_equal Plus (convert_plus_op ());
  assert_equal Minus (convert_minus_op ());
  assert_equal Multiply (convert_multiply_op ());
  assert_equal Divide (convert_divide_op ());
  assert_equal Equal (convert_equal_op ())

(** {1 åˆ†éš”ç¬¦è½¬æ¢å‡½æ•°æµ‹è¯•} *)

(** åˆ†éš”ç¬¦è½¬æ¢æµ‹è¯• *)
let test_delimiter_conversions _ =
  assert_equal LeftParen (convert_left_paren ());
  assert_equal RightParen (convert_right_paren ());
  assert_equal Comma (convert_comma ());
  assert_equal Semicolon (convert_semicolon ())

(** {1 ç‰¹æ®ŠTokenè½¬æ¢å‡½æ•°æµ‹è¯•} *)

(** ç‰¹æ®ŠTokenè½¬æ¢æµ‹è¯• *)
let test_special_token_conversions _ =
  assert_equal EOF (convert_eof ());
  assert_equal Newline (convert_newline ());
  
  let comment_text = "this is a comment" in
  assert_equal (Comment comment_text) (convert_comment comment_text);
  
  let whitespace_text = "   \t  " in
  assert_equal (Whitespace whitespace_text) (convert_whitespace whitespace_text)

(** {1 ç»Ÿä¸€Tokenæ„é€ å‡½æ•°æµ‹è¯•} *)

(** å­—é¢é‡Tokenæ„é€ æµ‹è¯• *)
let test_make_literal_token _ =
  let int_lit = convert_int_token 42 in
  let token = make_literal_token int_lit in
  assert_equal (Literal (IntToken 42)) token;
  
  let float_lit = convert_float_token 3.14 in  
  let token = make_literal_token float_lit in
  assert_equal (Literal (FloatToken 3.14)) token;
  
  let string_lit = convert_string_token "hello" in
  let token = make_literal_token string_lit in
  assert_equal (Literal (StringToken "hello")) token

(** æ ‡è¯†ç¬¦Tokenæ„é€ æµ‹è¯• *)
let test_make_identifier_token _ =
  let simple_id = convert_simple_identifier "test" in
  let token = make_identifier_token simple_id in
  assert_equal (Identifier (SimpleIdentifier "test")) token;
  
  let quoted_id = convert_quoted_identifier "quoted" in
  let token = make_identifier_token quoted_id in
  assert_equal (Identifier (QuotedIdentifierToken "quoted")) token

(** å…³é”®å­—Tokenæ„é€ æµ‹è¯• *)
let test_make_core_language_token _ =
  let let_kw = convert_let_keyword () in
  let token = make_core_language_token let_kw in
  assert_equal (CoreLanguage LetKeyword) token;
  
  let fun_kw = convert_fun_keyword () in
  let token = make_core_language_token fun_kw in
  assert_equal (CoreLanguage FunKeyword) token

(** æ“ä½œç¬¦Tokenæ„é€ æµ‹è¯• *)
let test_make_operator_token _ =
  let plus_op = convert_plus_op () in
  let token = make_operator_token plus_op in
  assert_equal (Operator Plus) token;
  
  let equal_op = convert_equal_op () in
  let token = make_operator_token equal_op in
  assert_equal (Operator Equal) token

(** åˆ†éš”ç¬¦Tokenæ„é€ æµ‹è¯• *)
let test_make_delimiter_token _ =
  let left_paren = convert_left_paren () in
  let token = make_delimiter_token left_paren in
  assert_equal (Delimiter LeftParen) token;
  
  let comma = convert_comma () in
  let token = make_delimiter_token comma in
  assert_equal (Delimiter Comma) token

(** ç‰¹æ®ŠTokenæ„é€ æµ‹è¯• *)
let test_make_special_token _ =
  let eof = convert_eof () in
  let token = make_special_token eof in
  assert_equal (Special EOF) token;
  
  let comment = convert_comment "test comment" in
  let token = make_special_token comment in
  assert_equal (Special (Comment "test comment")) token

(** {1 ä½ç½®ä¿¡æ¯å¤„ç†æµ‹è¯•} *)

(** ä½ç½®ä¿¡æ¯åˆ›å»ºæµ‹è¯• *)
let test_make_position _ =
  let pos = make_position ~line:10 ~column:5 ~offset:100 in
  assert_equal 10 pos.line;
  assert_equal 5 pos.column;
  assert_equal 100 pos.offset

(** å¸¦ä½ç½®Tokenåˆ›å»ºæµ‹è¯• *)
let test_make_positioned_token _ =
  let token = make_literal_token (convert_int_token 42) in
  let position = make_position ~line:1 ~column:1 ~offset:0 in
  let positioned = make_positioned_token ~token ~position ~text:"42" in
  
  assert_equal token positioned.token;
  assert_equal position positioned.position;
  assert_equal "42" positioned.text

(** {1 Tokenç±»åˆ«æ£€æŸ¥å·¥å…·æµ‹è¯•} *)

(** Tokenç±»åˆ«è·å–æµ‹è¯• *)
let test_get_token_category _ =
  let literal = make_literal_token (convert_int_token 42) in
  assert_equal CategoryLiteral (get_token_category literal);
  
  let identifier = make_identifier_token (convert_simple_identifier "test") in
  assert_equal CategoryIdentifier (get_token_category identifier);
  
  let keyword = make_core_language_token (convert_let_keyword ()) in
  assert_equal CategoryKeyword (get_token_category keyword);
  
  let operator = make_operator_token (convert_plus_op ()) in
  assert_equal CategoryOperator (get_token_category operator);
  
  let delimiter = make_delimiter_token (convert_left_paren ()) in
  assert_equal CategoryDelimiter (get_token_category delimiter);
  
  let special = make_special_token (convert_eof ()) in
  assert_equal CategorySpecial (get_token_category special)

(** Tokenç±»å‹æ£€æŸ¥å‡½æ•°æµ‹è¯• *)
let test_token_type_checks _ =
  let literal = make_literal_token (convert_int_token 42) in
  assert_bool "is_literal_token failed" (is_literal_token literal);
  assert_bool "is_identifier_token should be false" (not (is_identifier_token literal));
  
  let identifier = make_identifier_token (convert_simple_identifier "test") in
  assert_bool "is_identifier_token failed" (is_identifier_token identifier);
  assert_bool "is_literal_token should be false" (not (is_literal_token identifier));
  
  let keyword = make_core_language_token (convert_let_keyword ()) in
  assert_bool "is_keyword_token failed" (is_keyword_token keyword);
  assert_bool "is_operator_token should be false" (not (is_operator_token keyword));
  
  let operator = make_operator_token (convert_plus_op ()) in
  assert_bool "is_operator_token failed" (is_operator_token operator);
  assert_bool "is_delimiter_token should be false" (not (is_delimiter_token operator));
  
  let delimiter = make_delimiter_token (convert_left_paren ()) in
  assert_bool "is_delimiter_token failed" (is_delimiter_token delimiter);
  assert_bool "is_special_token should be false" (not (is_special_token delimiter));
  
  let special = make_special_token (convert_eof ()) in
  assert_bool "is_special_token failed" (is_special_token special);
  assert_bool "is_literal_token should be false" (not (is_literal_token special))

(** {1 è°ƒè¯•å’Œè¯Šæ–­å·¥å…·æµ‹è¯•} *)

(** Tokenç±»å‹åç§°æµ‹è¯• *)
let test_token_type_name _ =
  let test_cases = [
    (make_literal_token (convert_int_token 42), "Literal");
    (make_identifier_token (convert_simple_identifier "test"), "Identifier");
    (make_core_language_token (convert_let_keyword ()), "CoreLanguage");
    (make_operator_token (convert_plus_op ()), "Operator");
    (make_delimiter_token (convert_left_paren ()), "Delimiter");
    (make_special_token (convert_eof ()), "Special");
  ] in
  List.iter (fun (token, expected_name) ->
    assert_equal expected_name (token_type_name token)
  ) test_cases

(** Tokenç»Ÿè®¡åŠŸèƒ½æµ‹è¯• *)
let test_count_token_types _ =
  let tokens = [
    make_literal_token (convert_int_token 1);
    make_literal_token (convert_int_token 2);
    make_operator_token (convert_plus_op ());
    make_identifier_token (convert_simple_identifier "x");
    make_special_token (convert_eof ());
  ] in
  
  let counts = count_token_types tokens in
  let find_count name = 
    List.assoc name counts 
  in
  
  assert_equal 2 (find_count "Literal");
  assert_equal 1 (find_count "Operator");
  assert_equal 1 (find_count "Identifier");
  assert_equal 1 (find_count "Special")

(** {1 æ‰¹é‡å¤„ç†å·¥å…·æµ‹è¯•} *)

(** æ‰¹é‡å­—é¢é‡Tokenåˆ›å»ºæµ‹è¯• *)
let test_make_literal_tokens _ =
  let values = [
    ("int1", `Int 42);
    ("float1", `Float 3.14);
    ("string1", `String "hello");
    ("bool1", `Bool true);
  ] in
  
  let tokens = make_literal_tokens values in
  assert_equal 4 (List.length tokens);
  
  (* éªŒè¯ç¬¬ä¸€ä¸ªToken *)
  match List.hd tokens with
  | Literal (IntToken 42) -> ()
  | _ -> assert_failure "First token incorrect";
  
  (* éªŒè¯æœ€åä¸€ä¸ªToken *)
  match List.rev tokens |> List.hd with
  | Literal (BoolToken true) -> ()
  | _ -> assert_failure "Last token incorrect"

(** æ‰¹é‡æ ‡è¯†ç¬¦Tokenåˆ›å»ºæµ‹è¯• *)
let test_make_identifier_tokens _ =
  let names = ["var1"; "var2"; "function_name"; "ä¸­æ–‡å˜é‡"] in
  let tokens = make_identifier_tokens names in
  
  assert_equal 4 (List.length tokens);
  List.iter2 (fun name token ->
    match token with
    | Identifier (SimpleIdentifier id) -> assert_equal name id
    | _ -> assert_failure "Invalid identifier token"
  ) names tokens

(** {1 å®éªŒæ€§è½¬æ¢åŠŸèƒ½æµ‹è¯•} *)

(** å­—ç¬¦ä¸²æ¨æ–­Tokenç±»å‹æµ‹è¯• *)
let test_infer_token_from_string _ =
  (* æµ‹è¯•æ•´æ•°æ¨æ–­ *)
  (match infer_token_from_string "42" with
   | Some (Literal (IntToken 42)) -> ()
   | _ -> assert_failure "Integer inference failed");
  
  (* æµ‹è¯•æµ®ç‚¹æ•°æ¨æ–­ *)
  (match infer_token_from_string "3.14" with
   | Some (Literal (FloatToken 3.14)) -> ()
   | _ -> assert_failure "Float inference failed");
  
  (* æµ‹è¯•å…³é”®å­—æ¨æ–­ *)
  (match infer_token_from_string "let" with
   | Some (CoreLanguage LetKeyword) -> ()
   | _ -> assert_failure "Keyword inference failed");
  
  (* æµ‹è¯•ä¸­æ–‡å…³é”®å­—æ¨æ–­ *)
  (match infer_token_from_string "è®©" with
   | Some (CoreLanguage LetKeyword) -> ()
   | _ -> assert_failure "Chinese keyword inference failed");
  
  (* æµ‹è¯•æ“ä½œç¬¦æ¨æ–­ *)
  (match infer_token_from_string "+" with
   | Some (Operator Plus) -> ()
   | _ -> assert_failure "Operator inference failed");
  
  (* æµ‹è¯•æ ‡è¯†ç¬¦æ¨æ–­ *)
  (match infer_token_from_string "variable_name" with
   | Some (Identifier (SimpleIdentifier "variable_name")) -> ()
   | _ -> assert_failure "Identifier inference failed")

(** TokenæµéªŒè¯æµ‹è¯• *)
let test_validate_token_stream _ =
  let valid_tokens = [
    make_literal_token (convert_int_token 42);
    make_operator_token (convert_plus_op ());
    make_literal_token (convert_int_token 3);
    make_special_token (convert_eof ());
  ] in
  
  assert_bool "Valid token stream validation failed" 
    (validate_token_stream valid_tokens);
  
  (* æµ‹è¯•ç©ºTokenæµ *)
  assert_bool "Empty token stream validation failed"
    (validate_token_stream [])

(** {1 è¾¹ç•Œæ¡ä»¶å’Œé”™è¯¯å¤„ç†æµ‹è¯•} *)

(** ç©ºå­—ç¬¦ä¸²å¤„ç†æµ‹è¯• *)
let test_empty_string_handling _ =
  let empty_string_token = convert_string_token "" in
  assert_equal (StringToken "") empty_string_token;
  
  let empty_identifier = convert_simple_identifier "" in
  assert_equal (SimpleIdentifier "") empty_identifier

(** æå¤§å€¼å¤„ç†æµ‹è¯• *)
let test_large_values _ =
  (* æµ‹è¯•å¤§æ•´æ•° *)
  let large_int = max_int in
  let result = convert_int_token large_int in
  assert_equal (IntToken large_int) result;
  
  (* æµ‹è¯•é•¿å­—ç¬¦ä¸² *)
  let long_string = String.make 10000 'x' in
  let result = convert_string_token long_string in
  assert_equal (StringToken long_string) result

(** æ€§èƒ½å‹åŠ›æµ‹è¯• *)
let test_performance_stress _ =
  (* å¤§é‡Tokenè½¬æ¢æ€§èƒ½æµ‹è¯• *)
  let start_time = Sys.time () in
  
  for i = 1 to 1000 do
    let _ = convert_int_token i in
    let _ = convert_string_token (string_of_int i) in
    let _ = make_literal_token (convert_int_token i) in
    ()
  done;
  
  let end_time = Sys.time () in
  let duration = end_time -. start_time in
  
  (* æœŸæœ›åœ¨åˆç†æ—¶é—´å†…å®Œæˆ *)
  assert_bool "Performance test took too long" (duration < 1.0)

(** {1 å¾€è¿”è½¬æ¢ä¸€è‡´æ€§æµ‹è¯•} *)

(** åŸºç¡€å¾€è¿”è½¬æ¢æµ‹è¯• *)
let test_roundtrip_consistency _ =
  (* æµ‹è¯•æ•´æ•°å¾€è¿”è½¬æ¢ *)
  let original_int = 42 in
  let converted = convert_int_token original_int in
  let token = make_literal_token converted in
  (match token with
   | Literal (IntToken i) -> assert_equal original_int i
   | _ -> assert_failure "Integer roundtrip failed");
  
  (* æµ‹è¯•å­—ç¬¦ä¸²å¾€è¿”è½¬æ¢ *)
  let original_string = "test_string" in
  let converted = convert_string_token original_string in
  let token = make_literal_token converted in
  (match token with
   | Literal (StringToken s) -> assert_equal original_string s
   | _ -> assert_failure "String roundtrip failed")

(** {1 æµ‹è¯•å¥—ä»¶ç»„è£…} *)

let conversion_tests = "Basic Conversion Tests" >::: [
  "test_convert_int_token" >:: test_convert_int_token;
  "test_convert_float_token" >:: test_convert_float_token;
  "test_convert_string_token" >:: test_convert_string_token;
  "test_convert_bool_token" >:: test_convert_bool_token;
  "test_convert_chinese_number_token" >:: test_convert_chinese_number_token;
]

let identifier_tests = "Identifier Conversion Tests" >::: [
  "test_convert_simple_identifier" >:: test_convert_simple_identifier;
  "test_convert_quoted_identifier" >:: test_convert_quoted_identifier;
  "test_convert_special_identifier" >:: test_convert_special_identifier;
]

let keyword_tests = "Keyword Conversion Tests" >::: [
  "test_keyword_conversions" >:: test_keyword_conversions;
]

let operator_tests = "Operator Conversion Tests" >::: [
  "test_operator_conversions" >:: test_operator_conversions;
]

let delimiter_tests = "Delimiter Conversion Tests" >::: [
  "test_delimiter_conversions" >:: test_delimiter_conversions;
]

let special_tests = "Special Token Conversion Tests" >::: [
  "test_special_token_conversions" >:: test_special_token_conversions;
]

let construction_tests = "Token Construction Tests" >::: [
  "test_make_literal_token" >:: test_make_literal_token;
  "test_make_identifier_token" >:: test_make_identifier_token;
  "test_make_core_language_token" >:: test_make_core_language_token;
  "test_make_operator_token" >:: test_make_operator_token;
  "test_make_delimiter_token" >:: test_make_delimiter_token;
  "test_make_special_token" >:: test_make_special_token;
]

let position_tests = "Position Handling Tests" >::: [
  "test_make_position" >:: test_make_position;
  "test_make_positioned_token" >:: test_make_positioned_token;
]

let category_tests = "Token Category Tests" >::: [
  "test_get_token_category" >:: test_get_token_category;
  "test_token_type_checks" >:: test_token_type_checks;
]

let utility_tests = "Utility Function Tests" >::: [
  "test_token_type_name" >:: test_token_type_name;
  "test_count_token_types" >:: test_count_token_types;
]

let batch_tests = "Batch Processing Tests" >::: [
  "test_make_literal_tokens" >:: test_make_literal_tokens;
  "test_make_identifier_tokens" >:: test_make_identifier_tokens;
]

let experimental_tests = "Experimental Feature Tests" >::: [
  "test_infer_token_from_string" >:: test_infer_token_from_string;
  "test_validate_token_stream" >:: test_validate_token_stream;
]

let edge_case_tests = "Edge Case and Error Handling Tests" >::: [
  "test_empty_string_handling" >:: test_empty_string_handling;
  "test_large_values" >:: test_large_values;
  "test_performance_stress" >:: test_performance_stress;
]

let roundtrip_tests = "Roundtrip Consistency Tests" >::: [
  "test_roundtrip_consistency" >:: test_roundtrip_consistency;
]

(** ä¸»æµ‹è¯•å¥—ä»¶ *)
let suite = "Legacy Type Bridge Comprehensive Tests" >::: [
  conversion_tests;
  identifier_tests;
  keyword_tests;
  operator_tests;
  delimiter_tests;
  special_tests;
  construction_tests;
  position_tests;
  category_tests;
  utility_tests;
  batch_tests;
  experimental_tests;
  edge_case_tests;
  roundtrip_tests;
]

(** è¿è¡Œæµ‹è¯• *)
let () = run_test_tt_main suite