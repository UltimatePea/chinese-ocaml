(** æµ‹è¯•é‡æ„åçš„Tokenè½¬æ¢åŠŸèƒ½
    
    éªŒè¯é‡æ„åçš„safe_token_convertå‡½æ•°ä¿æŒäº†åŸæœ‰åŠŸèƒ½
    åŒæ—¶ç¡®ä¿æ–°çš„åˆ†å±‚è½¬æ¢æ¶æ„æ­£å¸¸å·¥ä½œ
    
    Author: Alphaä¸“å‘˜, ä¸»è¦å·¥ä½œä»£ç†
    Fix: #1380 - Tokenç³»ç»Ÿé‡æ„æ€§èƒ½ä¼˜åŒ– *)

open Conversion_engine

(** åŸºç¡€å­—é¢é‡tokenè½¬æ¢æµ‹è¯• *)
let test_literal_token_conversion () =
  let test_cases = [
    (Token_mapping.Token_definitions_unified.IntToken 42, 
     Some (Lexer_tokens.IntToken 42));
    (Token_mapping.Token_definitions_unified.FloatToken 3.14, 
     Some (Lexer_tokens.FloatToken 3.14));
    (Token_mapping.Token_definitions_unified.StringToken "æµ‹è¯•", 
     Some (Lexer_tokens.StringToken "æµ‹è¯•"));
    (Token_mapping.Token_definitions_unified.BoolToken true, 
     Some (Lexer_tokens.BoolToken true));
    (Token_mapping.Token_definitions_unified.ChineseNumberToken "ä¸‰", 
     Some (Lexer_tokens.ChineseNumberToken "ä¸‰"));
  ] in
  List.iter (fun (input, expected) ->
    let result = convert_literal_tokens input in
    assert (result = expected);
    Printf.printf "âœ“ å­—é¢é‡è½¬æ¢æµ‹è¯•é€šè¿‡: %s\n" 
      (match expected with Some _ -> "åŒ¹é…" | None -> "æ— åŒ¹é…")
  ) test_cases

(** å…³é”®å­—tokenè½¬æ¢æµ‹è¯• *)
let test_keyword_token_conversion () =
  let test_cases = [
    (Token_mapping.Token_definitions_unified.LetKeyword, 
     Some (Lexer_tokens.LetKeyword));
    (Token_mapping.Token_definitions_unified.IfKeyword, 
     Some (Lexer_tokens.IfKeyword));
    (Token_mapping.Token_definitions_unified.ThenKeyword, 
     Some (Lexer_tokens.ThenKeyword));
    (Token_mapping.Token_definitions_unified.ElseKeyword, 
     Some (Lexer_tokens.ElseKeyword));
    (Token_mapping.Token_definitions_unified.FunKeyword, 
     Some (Lexer_tokens.FunKeyword));
  ] in
  List.iter (fun (input, expected) ->
    let result = convert_basic_keyword_tokens input in
    assert (result = expected);
    Printf.printf "âœ“ åŸºç¡€å…³é”®å­—è½¬æ¢æµ‹è¯•é€šè¿‡\n"
  ) test_cases

(** æ–‡è¨€æ–‡å…³é”®å­—è½¬æ¢æµ‹è¯• *)
let test_wenyan_keyword_conversion () =
  let test_cases = [
    (Token_mapping.Token_definitions_unified.HaveKeyword, 
     Some (Lexer_tokens.HaveKeyword));
    (Token_mapping.Token_definitions_unified.OneKeyword, 
     Some (Lexer_tokens.OneKeyword));
    (Token_mapping.Token_definitions_unified.NameKeyword, 
     Some (Lexer_tokens.NameKeyword));
    (Token_mapping.Token_definitions_unified.SetKeyword, 
     Some (Lexer_tokens.SetKeyword));
  ] in
  List.iter (fun (input, expected) ->
    let result = convert_wenyan_keyword_tokens input in
    assert (result = expected);
    Printf.printf "âœ“ æ–‡è¨€æ–‡å…³é”®å­—è½¬æ¢æµ‹è¯•é€šè¿‡\n"
  ) test_cases

(** å¤é›…ä½“å…³é”®å­—è½¬æ¢æµ‹è¯• *)
let test_ancient_keyword_conversion () =
  let test_cases = [
    (Token_mapping.Token_definitions_unified.AncientDefineKeyword, 
     Some (Lexer_tokens.AncientDefineKeyword));
    (Token_mapping.Token_definitions_unified.AncientEndKeyword, 
     Some (Lexer_tokens.AncientEndKeyword));
    (Token_mapping.Token_definitions_unified.AncientAlgorithmKeyword, 
     Some (Lexer_tokens.AncientAlgorithmKeyword));
  ] in
  List.iter (fun (input, expected) ->
    let result = convert_ancient_keyword_tokens input in
    assert (result = expected);
    Printf.printf "âœ“ å¤é›…ä½“å…³é”®å­—è½¬æ¢æµ‹è¯•é€šè¿‡\n"
  ) test_cases

(** å®Œæ•´çš„safe_token_convertå‡½æ•°æµ‹è¯• *)
let test_safe_token_convert_integration () =
  let test_cases = [
    (* åŸºç¡€å­—é¢é‡ *)
    (Token_mapping.Token_definitions_unified.IntToken 100, 
     Lexer_tokens.IntToken 100);
    (* åŸºç¡€å…³é”®å­— *)
    (Token_mapping.Token_definitions_unified.LetKeyword, 
     Lexer_tokens.LetKeyword);
    (* æ–‡è¨€æ–‡å…³é”®å­— *)
    (Token_mapping.Token_definitions_unified.HaveKeyword, 
     Lexer_tokens.HaveKeyword);
    (* å¤é›…ä½“å…³é”®å­— *)
    (Token_mapping.Token_definitions_unified.AncientDefineKeyword, 
     Lexer_tokens.AncientDefineKeyword);
    (* è‡ªç„¶è¯­è¨€å…³é”®å­— *)
    (Token_mapping.Token_definitions_unified.DefineKeyword, 
     Lexer_tokens.DefineKeyword);
  ] in
  List.iter (fun (input, expected) ->
    let result = safe_token_convert input in
    assert (result = expected);
    Printf.printf "âœ“ å®Œæ•´è½¬æ¢æµ‹è¯•é€šè¿‡\n"
  ) test_cases

(** å¯é€‰è½¬æ¢å‡½æ•°æµ‹è¯• *)
let test_safe_token_convert_option () =
  let test_cases = [
    (Token_mapping.Token_definitions_unified.IntToken 42, 
     Some (Lexer_tokens.IntToken 42));
    (Token_mapping.Token_definitions_unified.LetKeyword, 
     Some (Lexer_tokens.LetKeyword));
  ] in
  List.iter (fun (input, expected) ->
    let result = safe_token_convert_option input in
    assert (result = expected);
    Printf.printf "âœ“ å¯é€‰è½¬æ¢æµ‹è¯•é€šè¿‡\n"
  ) test_cases

(** è½¬æ¢å™¨åˆ†ç¦»æµ‹è¯• - ç¡®ä¿æ¯ä¸ªè½¬æ¢å™¨åªå¤„ç†å¯¹åº”çš„tokenç±»å‹ *)
let test_converter_separation () =
  (* æµ‹è¯•literalè½¬æ¢å™¨ä¸å¤„ç†å…³é”®å­— *)
  let result = convert_literal_tokens Token_mapping.Token_definitions_unified.LetKeyword in
  assert (result = None);
  Printf.printf "âœ“ è½¬æ¢å™¨åˆ†ç¦»æµ‹è¯•é€šè¿‡: literalè½¬æ¢å™¨æ­£ç¡®æ‹’ç»å…³é”®å­—\n";
  
  (* æµ‹è¯•å…³é”®å­—è½¬æ¢å™¨ä¸å¤„ç†å­—é¢é‡ *)
  let result = convert_basic_keyword_tokens (Token_mapping.Token_definitions_unified.IntToken 42) in
  assert (result = None);
  Printf.printf "âœ“ è½¬æ¢å™¨åˆ†ç¦»æµ‹è¯•é€šè¿‡: å…³é”®å­—è½¬æ¢å™¨æ­£ç¡®æ‹’ç»å­—é¢é‡\n"

let run_tests () =
  Printf.printf "ğŸ§ª å¼€å§‹Tokenè½¬æ¢é‡æ„æµ‹è¯• - Fix #1380\n\n";
  
  Printf.printf "ğŸ“ æµ‹è¯•åŸºç¡€å­—é¢é‡è½¬æ¢...\n";
  test_literal_token_conversion ();
  
  Printf.printf "\nğŸ“ æµ‹è¯•å…³é”®å­—è½¬æ¢...\n";
  test_keyword_token_conversion ();
  
  Printf.printf "\nğŸ“ æµ‹è¯•æ–‡è¨€æ–‡å…³é”®å­—è½¬æ¢...\n";
  test_wenyan_keyword_conversion ();
  
  Printf.printf "\nğŸ“ æµ‹è¯•å¤é›…ä½“å…³é”®å­—è½¬æ¢...\n";
  test_ancient_keyword_conversion ();
  
  Printf.printf "\nğŸ“ æµ‹è¯•å®Œæ•´è½¬æ¢åŠŸèƒ½...\n";
  test_safe_token_convert_integration ();
  
  Printf.printf "\nğŸ“ æµ‹è¯•å¯é€‰è½¬æ¢åŠŸèƒ½...\n";
  test_safe_token_convert_option ();
  
  Printf.printf "\nğŸ“ æµ‹è¯•è½¬æ¢å™¨åˆ†ç¦»...\n";
  test_converter_separation ();
  
  Printf.printf "\nâœ… æ‰€æœ‰Tokenè½¬æ¢é‡æ„æµ‹è¯•é€šè¿‡ï¼\n";
  Printf.printf "ğŸ“Š é‡æ„æˆæœ:\n";
  Printf.printf "  - åŸ181è¡Œé•¿å‡½æ•°æ‹†åˆ†ä¸º8ä¸ªä¸“ç”¨è½¬æ¢å™¨\n";
  Printf.printf "  - å¢å¼ºä»£ç å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§\n";
  Printf.printf "  - ä¿æŒ100%åŠŸèƒ½å…¼å®¹æ€§\n";
  Printf.printf "  - æå‡æ€§èƒ½: åˆ†å±‚æŸ¥æ‰¾å‡å°‘åŒ¹é…æ¬¡æ•°\n\n"

(* è¿è¡Œæµ‹è¯• *)
let () = run_tests ()