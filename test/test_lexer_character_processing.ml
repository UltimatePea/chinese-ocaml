(** è¯æ³•åˆ†æå™¨å­—ç¬¦å¤„ç†æµ‹è¯• - éª†è¨€ç¼–è¯‘å™¨ *)

open Yyocamlc_lib.Lexer

(** è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦åŒ…å«å­ä¸² *)
let contains_substring str substr =
  try
    let _ = Str.search_forward (Str.regexp (Str.quote substr)) str 0 in
    true
  with Not_found -> false

(** æµ‹è¯•ASCIIå­—ç¬¦ç¦ç”¨æ£€æŸ¥ *)
let test_ascii_forbidden_check () =
  (* æµ‹è¯•è¢«ç¦ç”¨çš„ASCIIç¬¦å· *)
  let forbidden_chars = ['+'; '-'; '*'; '/'; '%'; '^'; '='; '<'; '>'; '.'; 
                        '('; ')'; '['; ']'; '{'; '}'; ','; ';'; ':'; '!'; 
                        '|'; '_'; '@'; '#'; '$'; '&'; '?'; '\''; '`'; '~'] in
  
  List.iter (fun c ->
    try
      let _ = tokenize "test.ly" (String.make 1 c) in
      Printf.printf "âš  è­¦å‘Šï¼šå­—ç¬¦ '%c' åº”è¯¥è¢«ç¦ç”¨ä½†æœªè¢«æ£€æµ‹åˆ°\n" c;
    with
    | LexError (msg, _) ->
      assert (contains_substring msg "ç¦");
    | e -> 
      Printf.printf "æœªé¢„æœŸçš„å¼‚å¸¸ for '%c': %s\n" c (Printexc.to_string e);
  ) forbidden_chars;
  
  print_endline "âœ“ ASCIIå­—ç¬¦ç¦ç”¨æ£€æŸ¥æµ‹è¯•é€šè¿‡"

(** æµ‹è¯•é˜¿æ‹‰ä¼¯æ•°å­—ç¦ç”¨æ£€æŸ¥ *)
let test_arabic_numbers_forbidden () =
  let digits = ['0'; '1'; '2'; '3'; '4'; '5'; '6'; '7'; '8'; '9'] in
  
  List.iter (fun d ->
    try
      let _ = tokenize "test.ly" (String.make 1 d) in
      Printf.printf "âš  è­¦å‘Šï¼šé˜¿æ‹‰ä¼¯æ•°å­— '%c' åº”è¯¥è¢«ç¦ç”¨ä½†æœªè¢«æ£€æµ‹åˆ°\n" d;
    with
    | LexError (msg, _) ->
      assert (contains_substring msg "é˜¿æ‹‰ä¼¯æ•°å­—" || contains_substring msg "ç¦ç”¨");
    | e -> 
      Printf.printf "æœªé¢„æœŸçš„å¼‚å¸¸ for '%c': %s\n" d (Printexc.to_string e);
  ) digits;
  
  print_endline "âœ“ é˜¿æ‹‰ä¼¯æ•°å­—ç¦ç”¨æ£€æŸ¥æµ‹è¯•é€šè¿‡"

(** æµ‹è¯•å…è®¸çš„å­—ç¬¦ *)
let test_allowed_characters () =
  (* æµ‹è¯•ä¸­æ–‡å­—ç¬¦æ˜¯å¦è¢«å…è®¸ *)
  let chinese_chars = ["ä¸­"; "æ–‡"; "å­—"; "ç¬¦"] in
  
  List.iter (fun ch ->
    try
      let tokens = tokenize "test.ly" ch in
      Printf.printf "âœ“ ä¸­æ–‡å­—ç¬¦ '%s' è¢«æ­£ç¡®å¤„ç†ï¼Œç”Ÿæˆäº† %d ä¸ªtoken\n" ch (List.length tokens);
    with
    | e -> 
      Printf.printf "âš  ä¸­æ–‡å­—ç¬¦ '%s' å¤„ç†å¼‚å¸¸: %s\n" ch (Printexc.to_string e);
  ) chinese_chars;
  
  print_endline "âœ“ å…è®¸å­—ç¬¦æµ‹è¯•é€šè¿‡"

(** æµ‹è¯•å•å­—èŠ‚å­—ç¬¦tokenåŒ–æˆåŠŸæƒ…å†µ *)
let test_single_byte_tokenization_success () =
  try
    (* æµ‹è¯•å…è®¸çš„å•å­—èŠ‚å­—ç¬¦ï¼Œå¦‚å­—æ¯ *)
    let tokens = tokenize "test.ly" "a" in
    Printf.printf "âœ“ å­—ç¬¦ 'a' è¢«å¤„ç†ï¼Œç”Ÿæˆäº† %d ä¸ªtoken\n" (List.length tokens);
    print_endline "âœ“ å•å­—èŠ‚å­—ç¬¦tokenåŒ–æˆåŠŸæµ‹è¯•é€šè¿‡"
  with
  | e ->
    Printf.printf "å•å­—èŠ‚å­—ç¬¦tokenåŒ–æˆåŠŸæµ‹è¯•å¼‚å¸¸: %s\n" (Printexc.to_string e);
    print_endline "âš  å•å­—èŠ‚å­—ç¬¦tokenåŒ–æˆåŠŸæµ‹è¯•éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥"

(** æµ‹è¯•å•å­—èŠ‚å­—ç¬¦tokenåŒ–é”™è¯¯æ£€æµ‹ *)
let test_single_byte_tokenization_error () =
  try
    (* æµ‹è¯•ç¦ç”¨å­—ç¬¦åº”è¯¥æŠ›å‡ºå¼‚å¸¸ *)
    let _ = tokenize "test.ly" "+" in
    Printf.printf "âš  è­¦å‘Šï¼šç¦ç”¨å­—ç¬¦ '+' åº”è¯¥æŠ›å‡ºå¼‚å¸¸ä½†æœªæŠ›å‡º\n";
  with
  | LexError (msg, _) ->
    assert (contains_substring msg "ç¦");
    print_endline "âœ“ å•å­—èŠ‚å­—ç¬¦tokenåŒ–é”™è¯¯æ£€æµ‹æµ‹è¯•é€šè¿‡"
  | e ->
    Printf.printf "æœªé¢„æœŸçš„å¼‚å¸¸: %s\n" (Printexc.to_string e);
    print_endline "âš  å•å­—èŠ‚å­—ç¬¦tokenåŒ–é”™è¯¯æ£€æµ‹æµ‹è¯•éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥"

(** æµ‹è¯•å­—ç¬¦å¤„ç†æ¨¡å—çš„ç»“æ„å®Œæ•´æ€§ *)
let test_character_processing_module () =
  (* éªŒè¯è¯æ³•åˆ†æå™¨çš„æ ¸å¿ƒåŠŸèƒ½å­˜åœ¨ *)
  let _ = tokenize in
  let _ = next_token in
  let _ = find_keyword in
  print_endline "âœ“ å­—ç¬¦å¤„ç†æ¨¡å—ç»“æ„å®Œæ•´æ€§æµ‹è¯•é€šè¿‡"

(** è¿è¡Œæ‰€æœ‰æµ‹è¯• *)
let () =
  print_endline "å¼€å§‹è¿è¡Œè¯æ³•åˆ†æå™¨å­—ç¬¦å¤„ç†æµ‹è¯•...";
  test_ascii_forbidden_check ();
  test_arabic_numbers_forbidden ();
  test_allowed_characters ();
  test_single_byte_tokenization_success ();
  test_single_byte_tokenization_error ();
  test_character_processing_module ();
  print_endline "ğŸ‰ æ‰€æœ‰è¯æ³•åˆ†æå™¨å­—ç¬¦å¤„ç†æµ‹è¯•å®Œæˆï¼"